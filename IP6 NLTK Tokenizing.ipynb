{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook introduces the NLTK. Happily this is also a package that comes with Anaconda (it is a huge package and takes a long time to install ordinarily). You will notice that it takes a while to load here as well.\n",
      "\n",
      "What is NLTK good for? Previously with the FOMC meeting transcripts we were able to parse them by speech (etc.) using a regular expressions that captured speaker names. But what if we wanted to parse by sentence? What would we use then? And what if we wanted to stem the document or exclude stopwords? Count word frequencies or stuff like that? Wanted to create our own term document matrix to import into RTextTools? NLTK!\n",
      "\n",
      "NLTK also includes machine learning algorithms and natural language parsers (which will extract verbs, proper names etc). It's really quite overwhelming. And away we go....\n",
      "\n",
      "As before need to import relevant packages and change to the directory where the data are located:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import os\n",
      "import re\n",
      "#import pickle\n",
      "import nltk\n",
      "import nltk.data\n",
      "import numpy #nltk dependencies\n",
      "import scipy\n",
      "import matplotlib\n",
      "#import textmining\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "%cd C:\\Users\\John\\Desktop\\Programs\\Pyth\\FOMC2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then create a list of filenames to use in bringing the meeting texts into the FOMC list"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filelist = [filename for filename in os.listdir(os.getcwd()) if re.search(r'(.*\\.htm.*$)', filename) != None]\n",
      "\n",
      "print len(filelist) \n",
      "print filelist[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "FOMC = [] #create an empty list\n",
      "for infile in filelist: #for the files in 'listing'\n",
      "\tinput = open(infile, 'r') # open them\n",
      "\tTemp = BeautifulSoup(input.read()) #read them in\n",
      "\tTemp = Temp.get_text() #get the text\n",
      "\tFOMC.append(Temp) #append the text into one long list 'FOMC'\n",
      "\n",
      "print FOMC[0][1:100] #first item in list should be the text of the first .htm document. len(FOMC) should equal len(listing)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we are going to parse the files in FOMC by sentence. To do this we load the built in sentence tokenizer for English. It is informed by machine learning results the recognize (and ignore) things that do not indicate the end of a sentence, such as the period at the of 'Mr.' Works darn well but not perfectly of course."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "nltk.download('punkt')\n",
      "sent_tokenize = nltk.data.load('nltk:tokenizers/punkt/english.pickle')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "OK. let's apply it and append the result to the list meeting_sents"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "meeting_sents =[]\n",
      "for meeting in FOMC:\n",
      "    meeting_sents.append(sent_tokenize.tokenize(meeting)) \n",
      "\n",
      "meeting_sents[1][3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are different tokenizers (e.g. keep the period at the end of the sentence or not?) see http://nltk.googlecode.com/svn/trunk/doc/howto/tokenize.html"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Want words to all be lower case? Since the focus is on the word, the for loop must call each word in each sentence of meeting_sents. split splits the sentences that are being called into individual words. In the next section we are going to exclude certain words. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "meeting_words=[]\n",
      "for sent in meeting_sents:\n",
      "    for word in sent:\n",
      "        meeting_words.append(word.lower().split())\n",
      "    \n",
      "meeting_words[6]\n",
      "len(meeting_words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "OK, let's start with stopwords. We may not want to include words like 'the' as features in our analysis. First, get and print the English stopword list. These are the words that will be excluded by default. You could also create your own stopwords list."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.download('stopwords')\n",
      "from nltk.corpus import stopwords\n",
      "print(stopwords.words('english'))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next create a new list (of words) that only includes non-stopwords and words longer than 3 characters. This can take a while for the file we are working with so we are going to just look at a portion of the results. \n",
      "\n",
      "Note also that this for loop is written as a 'list comprehension.' which is just a way to write a loop on a single line. Start at the far right and read backward and it should make sense. Lambda is a way to define an anonymous function within a list comprehension."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "meeting_words2=[]\n",
      "meeting_words2 = [filter(lambda word: word not in stopwords.words('english') and len(word) >3, word) for word in meeting_words[0:10]]\n",
      "meeting_words2[2][0:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, Stem words using the Porter Stemmer  http://tartarus.org/martin/PorterStemmer/  \n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Pstemmer = nltk.stem.PorterStemmer()\n",
      "meeting_words3 = []\n",
      "meeting_words3 = [[Pstemmer.stem(word) for word in sentence] for sentence in meeting_words2]\n",
      "#meeting_words3 = [Pstemmer.stem(word) for word in allWords2]\n",
      "meeting_words3[0:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To be continued in the Summarizing notepad"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}